{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chat Bot Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aS7xeBhxEAi"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzlLnbMYxZ90"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding, encoder_units, batch_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        self.enc_units = encoder_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding)\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform', kernel_regularizer=tf.keras.regularizers.L2(0.001))\n",
        "    \n",
        "    def call(self, inputs, hidden_state):\n",
        "        embedded_inputs = self.embedding(inputs)\n",
        "        enc_outputs, thought_vector = self.gru(embedded_inputs, initial_state=hidden_state)\n",
        "        return enc_outputs, thought_vector"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8w769d_MxaIT"
      },
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(Attention, self).__init__()\n",
        "        \n",
        "        self.enc_output_layer = tf.keras.layers.Dense(units, kernel_regularizer=tf.keras.regularizers.L2(0.001))\n",
        "        self.thought_layer    = tf.keras.layers.Dense(units, kernel_regularizer=tf.keras.regularizers.L2(0.001))\n",
        "        self.final_layer      = tf.keras.layers.Dense(1    , kernel_regularizer=tf.keras.regularizers.L2(0.001))\n",
        "        \n",
        "    def call(self, enc_outputs, thought_vector):\n",
        "        thought_matrix = tf.expand_dims(thought_vector, 1)\n",
        "        \n",
        "        scores = self.final_layer(tf.keras.activations.tanh(self.enc_output_layer(enc_outputs) + self.thought_layer(thought_matrix)))\n",
        "        attention_weights = tf.keras.activations.softmax(scores, axis=-1)\n",
        "        \n",
        "        attention_output = attention_weights * enc_outputs # Shape (batch_size, num_outputs, output_size)\n",
        "        attention_output = tf.reduce_sum(attention_output, axis=1) # New shape (batch_size, output_size)\n",
        "        \n",
        "        return attention_output, attention_weights"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZsTbwW_xaLU"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding, decoder_units, batch_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        self.dec_units = decoder_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding)\n",
        "        self.gru = tf.keras.layers.GRU(self.dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform', kernel_regularizer=tf.keras.regularizers.L2(0.001))\n",
        "        \n",
        "        self.attention = Attention(self.dec_units)\n",
        "        self.word_output = tf.keras.layers.Dense(vocab_size, kernel_regularizer=tf.keras.regularizers.L2(0.001))\n",
        "        \n",
        "    def call(self, inputs, enc_outputs, thought_vector):\n",
        "        attention_output, attention_weights = self.attention(enc_outputs, thought_vector)\n",
        "        #Shape of attention output (batch_size, size_of_embedding)\n",
        "        \n",
        "        embedded_inputs = self.embedding(inputs) #Shape (batch_size, num_words, size_of_embedding)\n",
        "        attention_output = tf.expand_dims(attention_output, 1) #Shape of attention output (batch_size, 1, size_of_embedding)\n",
        "        concat_inputs = tf.concat([attention_output, embedded_inputs], axis=-1)\n",
        "        \n",
        "        decoder_outputs, hidden_state = self.gru(concat_inputs) #Shape (batch_size, 1, size_of_embedding)\n",
        "        decoder_outputs = tf.reshape(decoder_outputs, (-1, decoder_outputs.shape[2])) #Shape (batch_size, size_of_embedding)\n",
        "        \n",
        "        final_outputs = self.word_output(decoder_outputs)\n",
        "        return final_outputs, hidden_state, attention_weights"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeQMMk1NxaOK"
      },
      "source": [
        "class Train:\n",
        "    def __init__(self):\n",
        "        self.optimizer = tf.keras.optimizers.Adam()\n",
        "        self.base_loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "        \n",
        "    def loss_function(self, y_real, y_pred):\n",
        "        base_mask = tf.math.logical_not(tf.math.equal(y_real, 0))\n",
        "        base_loss = self.base_loss_function(y_real, y_pred)\n",
        "        \n",
        "        mask = tf.cast(base_mask, dtype=base_loss.dtype)\n",
        "        final_loss = mask * base_loss\n",
        "        \n",
        "        return tf.reduce_mean(final_loss)\n",
        "    \n",
        "    def train_step(self, train_data, label_data, enc_hidden, encoder, decoder, batch_size, label_tokenizer):\n",
        "        loss = 0\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            enc_outputs, thought_vector = encoder(train_data, enc_hidden)\n",
        "            dec_hidden = thought_vector\n",
        "            dec_input = tf.expand_dims([label_tokenizer.word_index['<start>']] * batch_size, 1)\n",
        "            \n",
        "            for index in range(1, label_data.shape[1]):\n",
        "                outputs, dec_hidden, _ = decoder(dec_input, enc_outputs, dec_hidden)\n",
        "                \n",
        "                dec_input = tf.expand_dims(label_data[:, index], 1)\n",
        "                loss = loss + self.loss_function(label_data[:, index], outputs)\n",
        "        \n",
        "        word_loss = loss / int(label_data.shape[1])\n",
        "        \n",
        "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "        \n",
        "        return word_loss"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuQ0NcdkxaQ4"
      },
      "source": [
        "class Data_Preprocessing:\n",
        "    def __init__(self):\n",
        "        self.temp = None\n",
        "    \n",
        "    def get_data(self, path):\n",
        "        file = open(path, 'r').read()\n",
        "        lists = [f.split('\\t') for f in file.split('\\n')]\n",
        "        \n",
        "        questions = [x[0] for x in lists]\n",
        "        answers = [x[1] for x in lists]\n",
        "        \n",
        "        return questions, answers\n",
        "    \n",
        "    def process_sentence(self, line):\n",
        "        line = line.lower().strip()\n",
        "        \n",
        "        line = re.sub(r\"([?!.,])\", r\" \\1 \", line)\n",
        "        line = re.sub(r'[\" \"]+', \" \", line)\n",
        "        line = re.sub(r\"[^a-zA-Z?!.,]+\", \" \", line)\n",
        "        line = line.strip()\n",
        "        \n",
        "        line = '<start> ' + line + ' <end>'\n",
        "        return line\n",
        "    \n",
        "    def word_to_vec(self, inputs):\n",
        "        tokenizer = Tokenizer(filters='')\n",
        "        tokenizer.fit_on_texts(inputs)\n",
        "        \n",
        "        vectors = tokenizer.texts_to_sequences(inputs)\n",
        "        vectors = pad_sequences(vectors, padding='post')\n",
        "        \n",
        "        return vectors, tokenizer"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ywxz-5M3xaTo"
      },
      "source": [
        "data = Data_Preprocessing()\n",
        "\n",
        "questions, answers = data.get_data('/content/drive/MyDrive/chatbot.txt')\n",
        "\n",
        "questions = [data.process_sentence(str(sentence)) for sentence in questions]\n",
        "answers = [data.process_sentence(str(sentence)) for sentence in answers]\n",
        "\n",
        "train_vectors, train_tokenizer = data.word_to_vec(questions)\n",
        "label_vectors, label_tokenizer = data.word_to_vec(answers)\n",
        "\n",
        "max_length_train = train_vectors.shape[1]\n",
        "max_length_label = label_vectors.shape[1]\n",
        "\n",
        "batch_size = 64\n",
        "buffer_size = train_vectors.shape[0]\n",
        "embedding_dim = 256\n",
        "steps_per_epoch = buffer_size//batch_size\n",
        "units = 1024"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lN4hgpmuxaWH"
      },
      "source": [
        "vocab_train = len(train_tokenizer.word_index) + 1\n",
        "vocab_label = len(label_tokenizer.word_index) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drUsAA6VxaY5"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((train_vectors, label_vectors))\n",
        "dataset = dataset.shuffle(buffer_size)\n",
        "dataset = dataset.batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpeEw3J1xabq"
      },
      "source": [
        "encoder = Encoder(vocab_train, embedding_dim, units, batch_size)\n",
        "decoder = Decoder(vocab_label, embedding_dim, units, batch_size)\n",
        "trainer = Train()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXtcuN9fxaeX",
        "outputId": "664e9eb6-b179-4707-b19d-cdef3fdcd411"
      },
      "source": [
        "EPOCHS = 50\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    enc_hidden = tf.zeros((batch_size, units))\n",
        "    total_loss = 0\n",
        "    \n",
        "    for (batch_num, (train_data, label_data)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = trainer.train_step(train_data, label_data, enc_hidden, encoder, decoder, batch_size, label_tokenizer)\n",
        "        total_loss = total_loss + batch_loss\n",
        "        \n",
        "    print(f\"Epoch: {epoch}, Loss: {total_loss/steps_per_epoch}\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 1.8837599754333496\n",
            "Epoch: 2, Loss: 1.5605084896087646\n",
            "Epoch: 3, Loss: 1.4231245517730713\n",
            "Epoch: 4, Loss: 1.3119827508926392\n",
            "Epoch: 5, Loss: 1.1954503059387207\n",
            "Epoch: 6, Loss: 1.074123501777649\n",
            "Epoch: 7, Loss: 0.9473547339439392\n",
            "Epoch: 8, Loss: 0.8237296938896179\n",
            "Epoch: 9, Loss: 0.7156314849853516\n",
            "Epoch: 10, Loss: 0.6268181800842285\n",
            "Epoch: 11, Loss: 0.5481952428817749\n",
            "Epoch: 12, Loss: 0.48114675283432007\n",
            "Epoch: 13, Loss: 0.4167391359806061\n",
            "Epoch: 14, Loss: 0.3584301173686981\n",
            "Epoch: 15, Loss: 0.305441290140152\n",
            "Epoch: 16, Loss: 0.26490047574043274\n",
            "Epoch: 17, Loss: 0.2266109436750412\n",
            "Epoch: 18, Loss: 0.19302013516426086\n",
            "Epoch: 19, Loss: 0.16686499118804932\n",
            "Epoch: 20, Loss: 0.14566758275032043\n",
            "Epoch: 21, Loss: 0.13067014515399933\n",
            "Epoch: 22, Loss: 0.11748877167701721\n",
            "Epoch: 23, Loss: 0.10895022004842758\n",
            "Epoch: 24, Loss: 0.1022108644247055\n",
            "Epoch: 25, Loss: 0.0941915363073349\n",
            "Epoch: 26, Loss: 0.0912880226969719\n",
            "Epoch: 27, Loss: 0.08706516772508621\n",
            "Epoch: 28, Loss: 0.08469324558973312\n",
            "Epoch: 29, Loss: 0.08111446350812912\n",
            "Epoch: 30, Loss: 0.07993447780609131\n",
            "Epoch: 31, Loss: 0.0779891237616539\n",
            "Epoch: 32, Loss: 0.07758602499961853\n",
            "Epoch: 33, Loss: 0.07593169808387756\n",
            "Epoch: 34, Loss: 0.07507333904504776\n",
            "Epoch: 35, Loss: 0.07480360567569733\n",
            "Epoch: 36, Loss: 0.07355833053588867\n",
            "Epoch: 37, Loss: 0.07324303686618805\n",
            "Epoch: 38, Loss: 0.07176000624895096\n",
            "Epoch: 39, Loss: 0.07334662973880768\n",
            "Epoch: 40, Loss: 0.07460878044366837\n",
            "Epoch: 41, Loss: 0.07460427284240723\n",
            "Epoch: 42, Loss: 0.07342737168073654\n",
            "Epoch: 43, Loss: 0.07334527373313904\n",
            "Epoch: 44, Loss: 0.07385663688182831\n",
            "Epoch: 45, Loss: 0.07284881174564362\n",
            "Epoch: 46, Loss: 0.0723375454545021\n",
            "Epoch: 47, Loss: 0.07277070730924606\n",
            "Epoch: 48, Loss: 0.07141939550638199\n",
            "Epoch: 49, Loss: 0.07105527073144913\n",
            "Epoch: 50, Loss: 0.07311415672302246\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFtblu_nyFHf"
      },
      "source": [
        "class Chatbot:\n",
        "    def __init__(self, encoder, decoder, train_tokenizer, label_tokenizer, max_length_train, units):\n",
        "        self.train_tokenizer = train_tokenizer\n",
        "        self.label_tokenizer = label_tokenizer\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.units = units\n",
        "        self.data = Data_Preprocessing()\n",
        "        self.maxlen = max_length_train\n",
        "    \n",
        "    def clean_answer(self, answer):\n",
        "        answer = answer[:-1]\n",
        "        answer = ' '.join(answer)\n",
        "        return answer\n",
        "    \n",
        "    def predict(self, sentence):\n",
        "        sentence = self.data.process_sentence(sentence)\n",
        "        \n",
        "        sentence_mat = []\n",
        "        for word in sentence.split(\" \"):\n",
        "            try:\n",
        "                sentence_mat.append(self.train_tokenizer.word_index[word])\n",
        "            except:\n",
        "                return \"Could not understand that, can you re-phrase?\"\n",
        "        \n",
        "        sentence_mat = pad_sequences([sentence_mat], maxlen=self.maxlen, padding='post')\n",
        "        sentence_mat = tf.convert_to_tensor(sentence_mat)\n",
        "        \n",
        "        enc_hidden = [tf.zeros((1, self.units))]\n",
        "        encoder_outputs, thought_vector = self.encoder(sentence_mat, enc_hidden)\n",
        "        \n",
        "        dec_hidden = thought_vector\n",
        "        dec_input = tf.expand_dims([label_tokenizer.word_index['<start>']], 0)\n",
        "        \n",
        "        answer = []\n",
        "        for i in range(1, self.maxlen):\n",
        "            pred, dec_hidden, _ = decoder(dec_input, encoder_outputs, dec_hidden)\n",
        "            \n",
        "            word = self.label_tokenizer.index_word[np.argmax(pred[0])]\n",
        "            answer.append(word)\n",
        "            \n",
        "            if word == '<end>':\n",
        "                return self.clean_answer(answer)\n",
        "            \n",
        "            dec_input = tf.expand_dims([np.argmax(pred[0])], 0)\n",
        "        \n",
        "        return self.clean_answer(answer)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsdz4v0ayFKT"
      },
      "source": [
        "bot = Chatbot(encoder, decoder, train_tokenizer, label_tokenizer, max_length_train, units)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3_OjazeyFMz",
        "outputId": "08980fbf-39f4-4a70-af04-bafb37463e30"
      },
      "source": [
        "question = ''\n",
        "while True:\n",
        "    question = str(input('You:'))\n",
        "    if question == 'quit' or question == 'Quit':\n",
        "        break\n",
        "        \n",
        "    answer = bot.predict(question)\n",
        "    print(f'Bot: {answer}')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You:hi\n",
            "Bot: hi\n",
            "You:how are you?\n",
            "Bot: i am fine\n",
            "You:who are you?\n",
            "Bot: my name is jarvis\n",
            "You:do you like the rain?\n",
            "Bot: the sky looks so clean after it .\n",
            "You:that's nice\n",
            "Bot: are you going to make it s a lot of the world .\n",
            "You:i hate chocolates\n",
            "Bot: Could not understand that, can you re-phrase?\n",
            "You:which season do you like?\n",
            "Bot: the one .\n",
            "You:which one?\n",
            "Bot: any one .\n",
            "You:who invented flossing?\n",
            "Bot: a dentist , i m sure .\n",
            "You:do you like hawaii vacation?\n",
            "Bot: it .\n",
            "You:do you need some coffee?\n",
            "Bot: yes , thank you .\n",
            "You:what kind?\n",
            "Bot: a ham sandwich .\n",
            "You:do you like those?\n",
            "Bot: i like it .\n",
            "You:what is your first class?\n",
            "Bot: my very first class is human biology .\n",
            "You:are you the chosen one?\n",
            "Bot: no .\n",
            "You:how are you doing?\n",
            "Bot: i am fine\n",
            "You:who is your friend?\n",
            "Bot: no .\n",
            "You:no?\n",
            "Bot: i need to hire , it s an appointment to hire , it s an appointment to hire , it s an\n",
            "You:i type too much\n",
            "Bot: you should take a break a break a break a break a break a break a break a break a break a\n",
            "You:okay then\n",
            "Bot: i ll never been there .\n",
            "You:been where?\n",
            "Bot: i think we should get a history class in a history class in a history class in a history class in a\n",
            "You:take care\n",
            "Bot: oh , it .\n",
            "You:bye\n",
            "Bot: bye\n",
            "You:quit\n"
          ]
        }
      ]
    }
  ]
}